---
layout: post
title:  "Hadoop"
date:   2019-04-08
excerpt: "Hadoop"
tags: [Hadoop]
comments: true
---
数据量的爆炸性增长
数据处理速度缓慢

Hadoop 为我们提供了一个可靠的且可扩展的存储和分析平台。

HDFS 是 Hadoop 自带的分布式文件系统。

MapReduce 是一个批量查询处理器，能够在合理的时间范围内处理针对整个数据集的动态查询。

MapReduce 基本上是一个批量处理系统，并不适合交互式分析，更适合离线使用场景。

MapReduce 是一种计算模型，将大型数据处理业务分解成很多单个的、可以在服务器集群中并行执行的任务。

与 Hadoop 协同工作的各种处理模式

- 交互式 SQL（Interactive SQL）
	利用 MapReduce 进行分发并使用一个分布式查询引擎，使得在 Hadoop 上获得 SQL 查询低延迟响应的同时还能保持对大数据集规模的可扩展性。Hive。
- 迭代处理（Iterative processing）
	在内存中保存每次中间结果集。Spark。
- 流处理（Stream processing）
	在无边界数据流上运行实时、分布式计算。
- 搜索（Search）
	根据 HDFS 中存储的索引为搜索查询提供服务。Solr。

为什么不使用数据库进行大规模数据分析？因为寻址时间的提升远远低于传输速度的提升。

关系型数据库适用于索引后的数据集的点查询和更新，适合持续更新的数据集，MapReduce 适合一次写入、多次读取数据的应用，以批处理的方式分析整个数据集的问题。

关系型数据库存储结构化数据，数据规范以保持其数据的完整性且不含冗余。Hadoop对半结构化（电子表格）、非结构化（日志文件）可以进行有效处理。

MapReduce 是一种可用于数据处理的编程模型。本质上是并行运行，因此可以将大规模数据分析任务分发给任何一个拥有足够多机器的数据中心。

MapReduce 任务过程分为两个处理阶段：map 阶段和 reduce 阶段。每阶段都以键值对作为输入和输出。
通过使用资源管理系统 YARN 的调度，将 MapReduce 计算转移到存储到有部分数据的各台机器上，如果一个任务失败，将会在另一个不同的节点上自动重新调度运行。
Hadoop 将 MapReduce 的输入数据划分成等长的小数据块，称为输入分片，并为每个分片构建一个 map 任务运行用户定义的 map 函数处理分片数据。合理的分片大小趋向于 HDFS 的一个块的大小，默认 128 MB。

为什么分片数据趋向于块大小？因为如果大于块大小，一个分片数据将会在不同的 HDFS 节点上，会涉及到网络传输，导致效率低。所以最佳性能是在存储有输入数据的节点上运行相对应的 map 任务，不会占用带宽。

map 任务的输出会写入本地硬盘。因为 map 的输出是中间结果。
reduce 任务的输出存储在 HDFS 中以实现可靠存储。第一个副本存储在本地节点上，其他副本出于可靠性考虑存储在其他机架。

reduce 任务的数量需要独立指定，并不由输入数据的大小决定。

单 reduce 任务会合并处理各个 map 输出的排过序的数据。
多 reduce 任务情况下，每个 map 任务会为每个 reduce 任务建一个分区，使用哈希函数来分区，每个键对应的键值对记录都在同一个分区。因此，每个 reduce 任务输入都来自许多 map 任务，所以这个数据流传输过程称为混洗（shuffle）。

有一个优化方案，combiner 函数，适用于比较，不适用于求值。

HDFS 的块比磁盘的块大，其目的是为了最小化寻址开销。

fsck / -files -blocks

HDFS 集群有两类节点，以管理节点（namenode ）-工作节点（datanode）模式的运行。
namenode 管理文件系统的命名空间，维护文件系统树内所有的文件和目录，保存在本地磁盘上。同时记录着每个文件中各个块所在的数据节点信息，但并不永久保存块的位置信息，因为会在系统启动时根据数据节点信息重建。
datanode 受客户端或者 namenode 的调度来根据需要存储并检索数据块，并定期向 namenode 发送自身所存储的块的列表。

namenode 是文件系统正常运行的关键，一旦损毁，将无法通过 datanode 的块重建文件，文件系统上的所有文件都会丢失。因此，多 namenode、主从 namenode 或者热备份都是不错的选择。

通常 datanode 从磁盘中读取块，但对于访问频繁的文件，对应的块可能缓存在 datanode 的内存中，以堆外块缓存的形式存在。

YARN 是 Hadoop 的集群资源管理系统，提供请求和使用集群资源的 API。通过【管理集群上资源使用的资源管理器】和【运行在集群中所有节点上且能够启动和监控容器的节点管理器】这两类长期运行的守护进程提供核心服务。根据既定策略为应用分配资源。

Hadoop 以 Java API 的形式提供文件系统访问接口。

Hive 起步

建表语句
CREATE TABLE fi.ods_rrs_income_100(
  id INT,
  pay_order STRING,
  country_id INT,
  site_id INT,
  category_first INT,
  goods_line STRING,
  goods_sn STRING,
  send_time TIMESTAMP,
  pay_currency_id INT,
  pay_money DECIMAL,
  local_money DECIMAL,
  stats_flag TINYINT,
  invstats_flag TINYINT,
  remark STRING,
  opt_id INT,
  add_time TIMESTAMP,
  update_time TIMESTAMP,
  mark TINYINT,
  sub_site_code STRING,
  order_type TINYINT
)partitioned by (dt string comment 'yyyymmdd')
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LINES TERMINATED BY '\n';

不需要像SQL一样指定数据长度

注意没有 DATETIME 类型，有 DATE 类型（2012-12-01），不包含时间信息。

外部表：使用 EXTERNAL 关键字创建外部表
CREATE EXTERNAL TABLE xxxx(...)
在删除表结构时不会删除表数据

内部表的应用场景：存储临时数据、Hive是管理表和数据的生命周期的唯一方式
表的类型为 MANAGED_TABLE，删除时会删除底层数据

目前，创建ods表需要分区！
partitioned by (dt string comment 'yyyymmdd')
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '\t'
  LINES TERMINATED BY '\n';

分区和分桶的区别
分区根据特别的标记，将相同标记的数据放在一起，提高效率，需要在加载数据时显式指定分区
partition by(dt String)
dt非表字段
select ... where dt = xxx

分桶根据 hash 值将数据散列
clustered by(表字段) into 4 buckets

关系型数据库导入Hive，需要SQOOP，数据抽取工具，内置通用的jdbc连接器。
sqoop import --connect jdbc://mysql//.......
import 工具会运行一个mr任务，一般会将数据保存为逗号分隔的文本文件。

--where，导入控制，暂不支持。

sqoop create-hive-table --connect...,BI 这边是不是这样的

导入是将数据从RDMS传输到HDFS，导出是将数据从HDFS传输到RDMS，因为RDMS数据类型多，所以需要提前准备好数据表结构。

需要列相同，是不是说表结构要一样，不可以有间歇

metastore 概念

读时模式和写时模式

distribute by 字段名

hive 只支持等值连接

explain 语句输出某个查询使用多少mr

全外连接 full outer join .. on ..

半连接 left semi join 

还可以自定义函数

调取可用函数
show functions;


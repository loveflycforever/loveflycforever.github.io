TCP

第一层优化 硬件 硬盘、CPU
第二层优化 链路层 机房
第三层优化 路由层 middlemell选路
第四层优化 传输层 TCP
第七层优化 应用层 内容压缩

见  tcp传输图片

此处为  早期的TCP的拥塞控制算法

当TCP开始传输时，开始并不是以一个很快的速度发出数据包，实际在微观上它是慢慢把速度加起来的过程，只是这个时间很短无法察觉。从微观上来看，最开始是有一个所谓的初始发包数量的这么一个概念。在早期的2.6.18内核，也就是3645相对应的这个版本之前，初始的发包数量是两个。发出第一轮数据，实际上只发2个数据包，即拥塞窗口为2（CWND），等待这2个数据包完全确认，如这两个数据包完全收到ACK确认数据之后，会认为第一轮数据都已经收到了，这时它会把发包的数量调整成4个，如果第二个也收到了，接下来就变为为8个、16个、32个这样的一个**指数**增长过程，就是慢启动（ss，slow start）的过程，这个过程就是TCP刚开始建立的时候的最初始的必经的一个阶段。
在传输到一定速度的时候，为尽量的规避和避免网络的拥塞，增长方式变成**线性**的速度增长，称为慢启动阈值（ssthresh，slow start threshold）。
如果没有丢包，不会降速或者调整发送速度。当发生在丢包现象的时候，会记录此速度值，并且通过拥塞控制算法，算出一个合理的阈值。那么当下一次速度增长到这个阈值的时候，就会知道不能再指数增长了，而是应该线性的增长发包的数量，来避免再次丢包。
当出现RTO期间，即服务器发数据而客户端始终没有响应的时，会等待一个超时定时器。这个超时定时器一旦出现，就会回到另一个协议栈里的一个状态机。当状态机处于丢包状态时，就会把它的CWND降到最开始的大小，那么速度就会骤降，这个非常严重。并且一旦骤降它会重新评估。比如说CWND到达24时丢包了，然后算出应该到16就变成线性。如果再出现严重问题，可能会把阈值降到12，并启用线性传输模式了。

-----------------------------------------------------------------------------------------------

优化，
建连优化，贝司值原本设置为2的倍数或者3的倍数递增去重试，可以优化为失败后快速重发。
首包优化，首包时间是发完http请求之后所拿到的第一个数据，一般是响应头，因为不同的server的实现，这个响应头可能是和内容一起发送过来，也有可能只是先发送一个响应头然后再发内容。在TCP里面有一个Nagel算法，Nagel算法会把这个数据拼凑成一个大块儿后发出。如果是在engikers里配TCP nodelay，就会有什么发什么，可以来提升这个首包的效果。
平滑发包，充分的利用每一个发包周期之间的时间间隔，然后把数据包打散。这样的话，既不会从从宏观上感觉发送速度慢，从微观上也让这个数据变得更平滑，而不会导致某一个具体的小时间的一个时刻，由于链路不充足而导致丢包。
丢包预判，就是通过统计学的一些方法，记录端到端的传输具体情况，如果丢包率是比较符合规律的话，可以在发包前预判有可能丢包，就时不时的在对方能收到的情况下去把某些数据包重发一遍或者两遍，增加抗丢包效果。

---------------------------------------------------------------

很多做TCP优化的，只是改变TCP拥塞控制算法，直接编译出一个内核模块重新加载，然后改一下的拥塞控制模块，再重新加载一下就OK了。实际上，这种做法只能改变你计算CWND的数量，但并不能改变他的状态。如果你想做TCP优化，你必须要动TCP协议栈本身。

在linux协议栈里面有一个控制参数叫tcp slow start after idle。意思是，你在数据传输的时，如果等了一段时间超出一定的时间阈值之后他会把CWND给你降到初始值。

比如你是一个http的业务，而且都是小文件，恰好又用了keep live这种状态，每一个请求结束后并不马上断掉链接，而是期待下一次的数据请求。在第一个数据块object，比如下载第一个图片，他去请求时，这个CWND会逐渐通过慢系统长到一定的高度。由于你两次get请求之间可能间隔了一段时间，这个时间一旦超过阈值，就会在发送端自己把这个CWND降到初始值。一旦降到初始值，这个时候你第一个object在下载以后，CWND好不容易涨上去的就白长了。你在下载第二个的时候实际上还是从慢速开始。

在linux系统里默认这个值开启，它实际上是会给你降的。如果你想做优化的话，最简单的做法就是把它置成0。如果置成0，即使连接中间隔的时间很长，你在请求第二个object的时，他的初始的发送速度就继续按照刚才的大小继续发送。这样，就可以达到一个很好的加速效果。

--------------------------------------------------------

见RTT 1图

这是个在没有做CDN之前，客户访问源站的时候的连接示意图。这里面提到一个概念RTT，之前也说到了它的真正术语是往返时延，实际上就是我们平时说的ping值。但是ping值只是RTT的一部分，并不是说RTT就是ping值。实际上，TCP也有往返时延的，比如，你发起一个信包，然后对方回复，这段时间间隔也是RTT。有一种ping叫TCPping，利用的就是这个特点。如图，假设客户到原站的RTT是80毫秒，假设你要下载的文件是30kb,算一下他大概需要多久时间可以下载完成。
图里共3种颜色，红色代表TCP的三次握手是一个建连的过程。第一次，第二次第三次然后建连完成。大家仔细看绿色的，绿色是发get请求的时候的一个数据包。蓝色的大量的数据传输表示服务器开始以不同周期开始吐数据。那么这里边我是假设他初始CWND是2，那就是2、4、8这样去长。
在TCP里边还有一个MSS的概念，TCP协议能一个数据包存储的最大真正的七层内容是有多长。在普通的网络当中，MTU如果是1500字节的话，IP头是20字节，TCP头是20字节，在一般情况下MSS是1460，也就是说一个数据包可以传递1460字节的内容。那我们算一下，30kb是30*1024这么多字节，那么它需要几轮才能传输完成呢？
第一轮发两个数据包，第二轮发四个数据包，第三轮发八个数据包，第四轮的时候其实剩余的数据量已经不足16个数据包了，但是仍然要发送一轮。
后面是四个来回我刚才讲了，再加上前面的TCP三次握手也占一个往返时延。也就是说一共有五个来回周期。那假设这个往返时延是80毫秒，可以算一下，5*80，这是400毫秒。也就是在这么一个网络的情况下，带宽足够充足，然后在没有抖动也没有丢包的情况下，要传一个30kb的数据在80毫秒延迟的情况下,他最快的理论速度也需400毫秒才能完成，这是在CDN之前。


见RTT 2 图

上图是在做CDN之后的效果图。我们可以看到，在客户和园站之间，部署两个CDN的节点，一个叫下层一个叫上层。下层离用户很近，这一块儿的距离就lastmell。上层离源站近，这一块儿这个距离我们firstmell。然后上下层之间我们叫middlemell。为确保能够充分的体现即使是动态的数据我们也能起到很完美的加速效果，我在上下层之间我仍保留80毫秒。同时在Lastmell和firstmell分别引入20毫秒，那么总共延时就变成了120毫秒。也就是说现在网络环境总延时是原来的1.5倍。
首先来看一下firstmell，因为firstmell和加速之前的拓普相比，唯一不同的是往返时延变小，由80毫秒变成了20毫秒。计算一下20*（1+4）=100毫秒。
再来看lastmell，由于lastmell发送端是我们自己的服务器，所以完全可以利用TCP优化这种方式来让数据包发送更快。比如，最简单的加速方式就是去修改你的CWND，原来是2现在修改到10，目前在2.6.32内核以上，他默认的这个CWND应该都是10,早期是2。谷歌在很早之前就已提出一个观点，早期ARFC的标准里说初始值是2，,已不合时宜，因为现在的网络带宽都很大。所以我们应该把它提到10，后来就所有的东西都是以10为初始值。
假设初始值就是10，可以算一下需要多少个周期能把数据发完？共有30K的数据，第一轮发10个，每一个数据包记住这里边说的10个指的是10个数据包。每个数据包可存放的内容是1460个字节，那实际上第一轮发10个就已经变成14.6k。那第二轮20个数据包用不满就已经发完了，套用公式20*3，实际上只需要60毫秒就可完成。
最有意思的是middlemell这块。因为所有的东西全都是我们自己的，服务器也是我们自己的，然后网络也是相对来说可控的。所以可以在middlemell这块儿做很多有意思的事情。比如TCP，由于服务器是在机房，他跟原针不一样，原针有可能带宽很小，而在lastmell也不可能吐数据太快，如果太快你的最终客户端的网络又个瓶颈。但我们两个节点之间传输的时实际上是可以速度很快的，也可以直接把数据一次性的30个包传到下层，从上层传到下层。
除这个以外还有一个很重要的观点，我可以通过上下层的一种长连接的机制keeplive的一个机制，忽略TCP的3次握手。即使换不同用户去访问同一个下层，但因我上下层之间已经建立了一个所谓的通道，那么也可以让这个数据通过我这个通道直接把get请求发送到上层，上层把这个交给原针。这样减少一个往返。套用公式可以看一下80*（0+1），总共只需要80毫秒。
把三个部分一起加起来，可以算一下60+80+100=240。也就是说，这种环境下,在总的延时是原来1.5倍的情况下，完美的做到比原来提升40%的优化效果。在不能缓存的纯动态的情况下，我在中间的middlemell没有任何RTT减少的情况下，我的CDN架构给你带来的价值。
还有两个细节我没有说，第一个细节是，真正的我们找上下层的链路的时有可能会小于80毫秒。因为利用我们之前说的那个路由的最短路径的算法，有可能会找一个经过c点到达上层，总共的RTT可能会小于80毫秒或更小，实际上还能进一步的缩短时间。另外，这里讲的是上层拿到所有的数据之后才会给下层，下层拿到所有数据之后才会给用户，实际上他不会在所有数据收到之后才传输，他是随收随传的，所以这三个过程在时间的横轴上是有叠加的，就导致时间进一步缩短。

-------------------------------------------

只要你有对外提供的server，而且server的负载比较高都会遇到的一个syncookie的坑。给大家讲一下。在TCP的标准里有两个选项一个叫WScale一个是SACK。他并不是在RFC793里边出现的，他是在RFC1323里补充而出现的。


现在讲一下这个WScale什么东西。默认的情况下在标准的TCP协议,在早期的时候是没有WScale概念的。他在TCP的头部有一个16byte的空间来表示你能发送的最大字节数,一个周期能发送的最大字节数。那根据TCP的吞吐量的计算公式，吞吐量一定是小于等于最大发送窗口除以RTT的,这个RTT是以秒为单位。


之所以说小于等于是因为一般的情况下他是有可能有乱序或者抖动的。假如你的TCP协议传输时，RTT是100毫秒，假设网络之间没有丢包也没有乱序也没有抖动，且带宽是无限大的。套公式可知，64k除以100毫秒，也就是0.1，吞吐量最大是640k。即使你的带宽无限大，没有丢包，没有抖动，最大640k，就是这么严格。


大家可能觉得这个有点儿不可思议，为什么我们传输速度是远大于这个呢？因为在新的标准里引用WScale这个概念。在TCP三次握手的时候，客户端如果要支持这个选项的话，服务端被通知支持这个选项。如果服务端也支持，会记录下来说客户端支持，同时回应也支持。在客户端拿到第二次握手时，服务端就也把自己置成支持状态了。在数据传输的时，它是以2为底数，然后以WScale的这个n值为指数的一个滑动窗口递增值。


利用这个WScale是可把发送窗口的数量涨到很大的，比如说64k、128k、256k甚至更大。如果要这样再套公式，他的传输效果就会变得非常好了。



关于参数SACK，选择性应答，全称是Selective ACK。在你数据传输的时，没有这个选项他会怎么样呢？比如，要传10个数据包，只有第6个数据包丢掉了，那么在服务端收到ACK的时候他会知道，只收到了5，然后就没有其他信息了。这个时候他需要怎么做呢？需要把6到10重新发一遍。那会导致两个问题，第一，你的数据从开始到传完，速度就会变慢。第二个就是占用额外带宽把7到10进行一个没必要的重传。


同样的在TCP三次握手的时候写标记，并且两边都确认同时开启，如果要是都支持的话，在客户端反馈数据的时，他就会告诉服务端，收到连续的序号完整的到5，但是我还收到了7到10。服务端就可通过这两个信息进行拼接找到中间的空隙，就会知道只有6号丢掉了，他就只需传6就可以。为什么要强调这两个，可能是为后面那个syncookie的坑做铺垫。


接触过linux的人都知道在里面有一个叫syncookie的机制。是一种帮助你去防护一定量攻击的一种方法。那么它的原理是什么呢？在每一次数据正常建立的时它优先消耗一个叫连接队列的一个概念。等连接队列满了，如果你syncookie未开启，有新的请求过来，他就会把那个新丢掉，如果你有大量的这种假的建连数据包已充斥满了整个建连队列的，那么他就会导致拒绝服务。


那syncookie开启的情况下会怎么样呢？他会在协议栈之前自己伪造一个应答机制，并不是真正的协议栈去代应答第二次握手。同时他的第二次握手会携带一个算好的一个cookie值作为第三次握手的校验。如果他收到了第三次握手的校验值的会被认为是一个合法的建连，那么，他会把这个通过注入的方式，直接告诉你这个链接可以直接用了。那在前期syncookie当满的时候开始启动这个状态，他是不占用队列的，所以说他是一个非常好的防攻击的一个手段，但是他的防攻击的量不会很大，微量是可以的。


但坑也恰恰就在这。由于syncookie他不是标准的TCP协议栈，所以说他的支持，并不是非常的完备。等一段syncookie发出，他代应答的第二次握手并不携带WScale和SACK这个选项。就会让客户端误认为是不支持的，所以，后续的沟通就变得非常的低效。我们之前做过一个实验，在有一定量丢包而且大延时的情况下，你的速度可能只有300多k。后来查了很多资料发现确实是这个样子,而且我们做了很多的模拟时间。比如，都为syncookie出发的时，他速度确实就很快。


后来我们做了一个改动，在syncookie上，如果要是代应答的时，我们携带SACK的这个数据给客户，那后来建连的时都可以把这个功能用起来。用起来时我们在线上是真正的无环境试验可以提升大概25%到35%的服务质量。

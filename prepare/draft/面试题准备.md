tomcat 的性能调优无非是虚拟机参数的调整即xms和xmx，最大内存为可用内存80%，设置在catalina.bat，JAVA_OPTS='-Xms256mXmx512m' 



-Xmx1024M-Xms512MXX:MaxPermSize=256m  



MaxPermSize=256m  持久代大小256m



消除DNS查询影响，修改server.xml的Connector标签属性enableLookups参数

web server 允许的最大连接数受制于内核，windows 2000 linux 1000



<Connector/> 标签节点属性

maxThreads 最大线程数

acceptCount 最大等待数

minSpareThreadsTomcat 初始化数

maxSpareThreads 开始关闭不再使用的socket的阈值

connnection Timeout 超时时间



静态页面进行缓存，使用Nginx，减少后端tomcat的访问，开启gzip，还有图片压缩



开启集群，需要服务支持共享session或者有用户服务器oauth



为什么要加大tomcat内存，肯定是程序提示java.lang.OutOfMemoryError:Java heap space，首先要检查是不是代码的问题，比如说死循环或者没有分批加载数据之类的







tomcat禁止列目录下文件

{ tomcat_home}/conf/web.xml  

```
<init-param>
<param-name>listings</param-name>
<param-value>false</param-value>
</init-param>
```

类加载是动态的，保证程序运行的基础类完全加载到JVM，其他的在有需要的时候才加载



GC时机，JVM空闲和当前堆空间不足。



finalize()，Java的析构函数，在内存充足的情况下，可能永远都不会执行，主要是回收特殊渠道(JNI调用non-java程序C、C++)申请的内存



如果被置为null，不会立即回收，在下一个回收周期



分布式垃圾回收DGC，利用引用计数法来做RMI涉及到的跨虚拟机远程对象的回收进行内存管理





垃圾回收不会发生在永久代，但是满了会触发full-gc





分布式系统

网络问题、延迟开销、带宽、安全

服务发现、服务注册

冗余问题

负载均衡

性能

部署复杂性





Eureka

nginx

Hystrix

Feign

Ribbon

Spring Cloud Bus 提供跨多实例刷新配置的功能







CSRF攻击 CSRF跨站请求伪造，针对改变状态的请求而不是数据窃取，因为攻击者无法看到响应数据



websocket，初始使用http然后升级到基于套接字的连接





dubbo zookeeper全挂了会利用本地缓存，服务提供者全挂了，消费者会无限次重连来等待恢复



dubbo负载均衡策略，随机（random）、轮询（roundrobin）、最少活跃（leastactive）、一致性哈希（缺省对第一个参数hash，key="hash.nodes“，默认160个虚拟节点）



doubo安全机制，token令牌，黑白名单



默认failover，出现错误，重试其他服务器

failfast，一次调用，失败报错，适用非幂等操作

Failsafe 出现异常，直接忽略，或写入日志

failback 失败自动恢复，定时重发，用于消息通知等操作

Forking 并行调用





dubbo协议采用单一长连接，每条连接最多能压满7mbyte数据，服务提供者千兆网卡（1024Mbit=128Mbyte）可以支持20个消费者

同时如果传大包（500kByte）的话，单个服务提供者的tps（每秒处理事务数）为262（128mbyte/500kbyte），消费者tps为14（7mbyte/500kbyte）

建议小于100k



为什么使用异步单一长连接：服务提供者少，消费者多，假设每日有1.5亿次调用，常规hessian容易被压垮，使用异步单一长连接，减少了握手验证，并异步IO复用线程池，防止C10K





### C10K问题

### 问题的解决方案：同一个线程/进程同时处理多个连接(I/O多路复用)

select方式：使用fd_set结构体告诉内核同时监控那些文件句柄，使用逐个排查方式去检查是否有文件句柄就绪或者超时。该方式有以下缺点：文件句柄数量是有上线的，逐个检查吞吐量低，每次调用都要重复初始化fd_set。

poll方式：该方式主要解决了select方式的2个缺点，文件句柄上限问题(链表方式存储)以及重复初始化问题(不同字段标注关注事件和发生事件)，但是逐个去检查文件句柄是否就绪的问题仍然没有解决。

epoll方式：该方式可以说是C10K问题的killer，他不去轮询监听所有文件句柄是否已经就绪。epoll只对发生变化的文件句柄感兴趣。其工作机制是，使用"事件"的就绪通知方式，通过epoll_ctl注册文件描述符fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd, epoll_wait便可以收到通知, 并通知应用程序。而且epoll使用一个文件描述符管理多个描述符,将用户进程的文件描述符的事件存放到内核的一个事件表中, 这样数据只需要从内核缓存空间拷贝一次到用户进程地址空间。而且epoll是通过内核与用户空间共享内存方式来实现事件就绪消息传递的，其效率非常高。但是epoll是依赖系统的(Linux)。

异步I/O以及Windows，该方式在windows上支持很好，这里就不具体介绍啦。













由于Java层面的线程与操作系统的原生线程有映射关系,如果要将一个
线程进行阻塞或唤起都需要操作系统的协助,这就需要从用户态切换到内
核态来执行,这种切换代价十分昂贵,很耗处理器时间,现代JDK中做
了大量的优化。一种优化是使用自旋锁,即在把线程进行阻塞操作之前先
让线程自旋等待一段时间，可能在等待期间其他线程已经解锁，这时就无
需再让线程执行阻塞操作,避免了用户态到内核态的切换。







锁消除:指虚拟机即时编译器在运行时,对一些代码上要求同步,但被检
测到不可能存在共享数据竞争的锁进行消除。主要根据逃逸分析。
程序员怎么会在明知道不存在数据竞争的情况下使用同步呢?很多不是程
序员自己加入的
锁粗化:原则上,同步块的作用范围要尽量小。但是如果一系列的连续操
作都对同一个对象反复加锁和解锁,甚至加锁操作在循环体内,频繁地进
行互斥同步操作也会导致不必要的性能损耗。
锁粗化就是增大锁的作用域。





CAS缺点:
1.乐观锁只能保证一个共享变量的原子操作。如果多一个或几个变量,乐
观锁将变得力不从心,但互斥锁能轻易解决,不管对象数量多少及对象颗
粒度大小。
2.长时间自旋可能导致开销大。假如CAS长时间不成功而一直自旋,会
给CPU带来很大的开销
3.ABA问题。CAS的核心思想是通过比对内存值与预期值是否一样而判
断内存值是否被改过,但这个判断逻辑不严谨，假如内存值原来是A,后
来被一条线程改为B,最后又被改成了A,则CAS认为此内存值并没有发
生改变,但实际上是有被其他线程改过的,这种情况对依赖过程值的情景
的运算结果影响很大。解决的思路是引入版本号，每次变量更新都把版本
号加一







锁的实现原理基本是为了达到一个目的:让所有的线程都能看到某
种标记。
Synchronized通过在对象头中设置标记实现了这一目的，是一种JVM原
生的锁实现方式,而ReentrantLock以及所有的基于Lock接口的实现
类,都是通过用一个volitile修饰的int型变量,并保证每个线程都能拥有
对该int的可见性和原子修改,其本质是基于所谓的AQS框架







基于AQS来构建。
1 AQS在内部定义了一个vOlatile int state变量,表示同步状态:当线程调
用lock方法时,如果state=0,说明没有任何线程占有共享资源的
锁，可以获得锁并将state=1;如果state=1,则说明有线程目前正在使
用共享变量,其他线程必须加入同步队列进行等待。
2.AQS通过Node内部类构成的一个双向链表结构的同步队列,来完成线
程获取锁的排队工作,当有线程获取锁失败后，就被添加到队列末尾
Node类是对要访问同步代码的线程的封装,包含了线程本身及其状
态叫
waitStatus(有五种不同取值,分别表示是否被阻塞,是否等待唤醒,是
否已经被取消等),每个Node结点关联其prev结点和next结点,方便
线程释放锁后快速唤醒下一个在等待的线程,是一个FIFO的过程。
Node类有两个常量,SHARED和EXCLUSIVE,分别代表共享模式和
独占模式。所谓共享模式是一个锁允许多条线程同时操作(信号量
Semaphore就是基于AQS的共享模式实现的),独占模式是同一个时
间段只能有一个线程对共享资源进行操作,多余的请求线程需要排队等
待(如ReentranLock)
3.AQS通过内部类ConditionObject构建等待队列(可有多个),当
Condition调用waitO方法后，线程将会加入等待队列中,而当
Condition调用signalO方法后,线程将从等待队列转移动同步队列中进
行锁竞争
4.AQS和Condition各自维护了不同的队列,在使用Lock和Condition
的时候，其实就是两个队列的互相移动。





ReentrantLock是Lock的实现类,是一个互斥的同步锁。从功能角度,

ReentrantLock比Synchronized的同步操作更精细(因为可以像普通对象一

样使用),甚至实现Synchronized没有的高级功能，如:

等待可中断:当持有锁的线程长期不释放锁的时候，正在等待的线程可以

选择放弃等待,对处理执行时间非常长的同步块很有用

带超时的获取锁尝试:在指定的时间范围内获取锁，如果时间到了仍然无

法获取则返回

可以判断是否有线程在排队等待获取锁。

可以响应中断请求:与Synchronized不同,当获取到锁的线程被中断

时，能够响应中断,中断异常将会被抛出,同时锁会被释放。

可以实现公平锁。

从锁释放角度，Synchronized在JVM层面上实现的，不但可以通过一些

监控工具监控Synchronized的锁定,而且在代码执行出现异常时,JVM

会自动释放锁定;但是使用Lock是通过代码实现的，要保证

锁定一定会被释放，就必须将unLockO放到finally(中

从性能角度,Synchronized早期实现比较低效,对比ReentrantLock,大

多数场景性能都相差较大。

但是在Java 6中对其进行了非常多的改进,在竞争不激烈时,

Synchronized的性能要优于ReetrantLock;在高竞争情况下,

Synchronized的性能会下降几十倍,但是ReetrantLock的性能能维持常

态







ReentrantLock是如何实现可重入性的?

ReentrantLock内部自定义了同步器Sync(Sync既实现了AQS,

又实现了

AOS,而AOS提供了一种互斥锁持有的方式,其实就是加锁的时候通过

CAS算法,将线程对象放到一个双向链表中，每次获取锁的时候，看下

当前维护的那个线程ID和当前请求的线程D是否一样，一样就可重入

了







JUC 包下

\countdownlatch\cyclicbarrier\semaphore\concurrenthashmap 同步工具

\concurrentskiplistmap\copyonwritearraylist 线程安全容器

ArrayBlockingQueue、SynchorousQueue 并发队列

Executor线程池工具









ReentrantLock-》ReadWriteLock-》StampedLock





ThreadLocal需要注意些什么?

使用ThreadLocal要注意remove!

ThreadLocal的实现是基于一个所谓的ThreadLocalMap,在

ThreadLocalMap中,它的key是一个弱引用

通常弱引用都会和引用队列配合清理机制使用,但是ThreadLocal是个例

外，它并没有这么做

这意味着,废弃项目的回收依赖于显式地触发,否则就要等待线程结束,

进而回收相应ThreadLocalMap!这就是很多OOM的来源,所以通常都会

建议,应用一定要自己负责remove,并且不要和线程池配合,因为

WOrker线程往往是不会退出的





1.构造器注入
2.Setter方法注入
3.接口注入



Spring提供了以下5中标准的事件:
1.上下文更新事件(ContextRefreshedEvent):该事件会在ApplicationContext
被初始化或者更新时发布。也可以在调用ConfigurableApplicationContext
接口中的refresh0方法时被触发
O
2.上下文开始事件(ContextStartedEvent):当容器调用
ConfigurableApplicationContext的Start0方法开始/重新开始容器时触发该
事件.
3.上下文停止事件(ContextStoppedEvent):当容器调用
ConigurableApplicationContext的Stop0方法停止容器时触发该事件
4.上下文关闭事件(ContextClosedEvent):当ApplicationContext被关闭时触
发该事件。容器被关闭时,其管理的所有单例Bean都被销毁
5.请求处理事件(RequestHandledEvent):在Web应用中,当一个http请求
(request)结束触发该事件
除了上面介绍的事件以外，还可以通过扩展ApplicationEvent类来开发自
定义的事件。







zookeeper 提供了文件系统、通知机制

为保证高吞吐和低延迟，在内存里维护了树状目录结构，每个节点的存放数据上限为1M

znode类型

1、
PERSISTENT-持久化目录节点
客户端与zookeeper断开连接后,该节点依旧存在
2 PERSISTENT SEQUENTIAL-持久化顺序编号目录节点
客户端与zookeeper断开连接后,该节点依旧存在，只是Zookeeper给该
节点名称进行顺序编号
3 EPHEMERAL-临时目录节点
客户端与zookeeper断开连接后,该节点被删除
4、
EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点
客户端与zookeeper断开连接后,该节点被删除,只是Zookeeper给该节
点名称进行顺序编号







redis和memcache比较

redis支持复杂数据结构，哈希，列表，集合。有序集合，场景：用户订单列表，用户消息，帖子评论

redis可以持久化数据，但是不要作为可靠的数据源，对于只读或者一致性要求不高

redis支持集群、主动复制、读写分离，mecache需要二次开发

redis值最大为1G，字符串最大512M

mecache纯KV，值最大为1M
1、纯KV,数据量非常大的业务,使用memcache更合适,原因是,
a)memcache的内存分配采用的是预分配内存池的管理方式,能够省去内
存分配的时间,redis是临时申请空间,可能导致碎片化。
b)虚拟内存使用,memcache将所有的数据存储在物理内存里,redis有自
己的Vm机制,理论上能够存储比物理内存更多的数据，当数据超量时,
引发swap,把冷数据刷新到磁盘上,从这点上,数据量大时,memcache
更快
c)网络模型,memcache使用非阻塞的1O复用模型,redis也是使用非阻
塞的Q复用模型,但是redis还提供了一些非KV存储之外的排序，聚合
功能,复杂的CPU计算,会阻塞整个l0调度,从这点上由于redis提供
的功能较多,memcache更快些
d)线程模型,memcache使用多线程,主线程监听,worker子线程接受请
求,执行读写,这个过程可能存在锁冲突。redis使用的单线程,虽然无锁
冲突,但是难以利用多核的特性提升吞吐量。





缓存不一致情况：产生原因，主从分离，读写分离导致的，解决方案：从库更新数据后，发出通知，更新缓存，但是伴随问题是是需要指定哪些特殊需求



主从不一致情况：产生原因，同步时差，主库高使用，解决方案，忽略不一致，让主库高可用只读主库，加缓存







Redis常见的性能问题和解决方案
1、master最好不要做持久化工作,如RDB内存快照和AOF日志文件
2、如果数据比较重要,某个Slave开启AOF备份,策略设置成每秒同步
一次
3、为了主从复制的速度和Slave最好在一个局
域网内
4、尽量避免在压力大得主库上增加从库
5、主从复制不要采用网状结构,尽量是线性结构,Master<--Slave1<----
Slave2







Redis的数据淘汰策略有哪些
voltile-lru从已经设置过期时间的数据集中挑选最近最少使用的数据淘汰
voltile-ttl从已经设置过期时间的数据库集当中挑选将要过期的数据
voltile-random从已经设置过期时间的数据集任意选择淘汰数据alkeys-lru
从数据集中挑选最近最少使用的数据淘汰
alkeys-random从数据集中任意选择淘汰的数据no-eviction禁止驱逐数据

noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被
使用的命令(大部分的写入指令，但DEL和几个例外)
allkeys-ru:尝试回收最少使用的键(LRU),使得新添加的数据有空间存放。
volatile-lru:尝试回收最少使用的键(LRU),但仅限于在过期集合的键,使得
新添加的数据有空间存放。
alkeys-random:回收随机的键使得新添加的数据有空间存放。
volatile-random:回收随机的键使得新添加的数据有空间存放,但仅限于在
过期集合的键。
volatile-tt!:回收在过期集合的键,并且优先回收存活时间(TTL)较短的键,使
得新添加的数据有空间存放。





找大量Key千万不要用keys命令，因为redis单线程，但是可以使用scan命令，但是有一定重复率，客户端自己做一次去重，但是时间长







redis并不是强一致性







redis最大节点数16384







Mongo更新操作立刻fsync到磁盘?
不会,磁盘写操作默认是延迟执行的。写操作可能在两三秒(默认在60秒
内)后到达磁盘。例如，如果一秒内数据库收到一千个对一个对象递增的操
作,仅刷新磁盘一次。(注意,尽管fsync选项在命令行和经过
getLastError_old是有效的)(译者:也许是坑人的面试题??)。





没有事务，类比于mysql mtisam的自动提交，写操作在2-3秒后刷入磁盘











当线程并发执行时，由于资源共享和线程协作，使用线程之间会存在以下两种制约关系。

1. 间接相互制约。一个系统中的多个线程必然要共享某种系统资源，如共享 CPU，共享 I/O 设备，所谓间接相 互制约即源于这种资源共享，打印机就是最好的例子，线程 A 在使用打印机时，其它线程都要等待。 

2. 直接相互制约。这种制约主要是因为线程之间的合作，如有线程 A 将计算结果提供给线程 B 作进一步处理， 那么线程 B 在线程 A 将数据送达之前都将处于阻塞状态。

   

   间接相互制约可以称为互斥，直接相互制约可以称为同步，对于互斥可以这样理解，线程 A 和线程 B 互斥访问某 个资源则它们之间就会产个顺序问题——要么线程 A 等待线程 B 操作完毕，要么线程 B 等待线程操作完毕，这其实就 是线程的同步了。因此同步包括互斥，互斥其实是一种特殊的同步。











常用的并发队列有阻塞队列和非阻塞队列，前者使用锁实现，后者则使用 CAS 非阻塞算法实现。



无法向一个 BlockingQueue 中插入 null。如果你试图插入 null，BlockingQueue 将会抛出一个 NullPointerException.